{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Tree-Based Methods & Ensembling Techniques (Notebook Development In Progress...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intro"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<br /><br />\n",
    "\n",
    "### Motivation\n",
    "\n",
    "Most of the models we've learned so far (linear/logistic regression, svm) and models we will learn in the upcoming classes (neural networks) involve <b>gradient-based weight optimization (using SGD).</b>\n",
    "\n",
    "In this class we will go through tree-based models, known as decision trees, that are different in a sense that they <b>do not require</b> weight optimization via SGD. Besides, tree-based models are <b>nonlinear</b>.\n",
    "\n",
    "Besides, we will learn about model ensembling - powerful technique in ML of combining multiple models in different ways in order to get much powerful models. Particularly, we will examine two most popular and widely used ways of decision tree ensembling - <b>Random Forests</b> and <b>Gradient Boosting</b>.\n",
    "\n",
    "As usual, there will be accompanying examples in Scikit-Learn."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "### Learning Objectives\n",
    "\n",
    "At the end of this class, listeners will be able to:\n",
    "\n",
    "<ul>\n",
    "    <li>Understand key concepts of decision trees and their ensembles.</li>\n",
    "    <li>Understand intuition behind different types of model ensembling.</li>\n",
    "    <li>Implement simple models using Scikit-Learn and Python.</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Reading Material (ordered by difficulty)\n",
    "\n",
    "Decision Trees (for Classification and Regression)\n",
    "\n",
    "<ul>\n",
    "    <li>https://www.youtube.com/watch?v=GZuweldJWrM&list=PLnZuxOufsXnvftwTB1HL6mel1V32w0ThI&index=20 - Bloomberg's ML lecture explaining decision trees for classification and regression. (<b>recommended prerequisite for this class</b>)</li>\n",
    "    <li>https://scikit-learn.org/stable/modules/tree.html - Good explanation of Tree methods with Cons and Pros and practical tips</li>\n",
    "    <li>T. Hastie, R. Tibshirani and J. Friedman -  Elements of Statistical Learning, SECTION 9.2 - In-depth explanation of theory behind classification/regression trees.</li>\n",
    "</ul>\n",
    "\n",
    "Ensembling\n",
    "\n",
    "<ul>\n",
    "    <li>\n",
    "        https://www.youtube.com/watch?v=f2S4hVs-ESw&list=PLqIj-nyfGu55TQpzjv_kwZcz3pJSW9bnH&index=22 - Bloomberg's ML lecture explaining tree ensembles. (<b>recommended prerequisite for this class</b>)\n",
    "    </li>\n",
    "    <li>http://blog.kaggle.com/2017/01/23/a-kaggle-master-explains-gradient-boosting/</li>\n",
    "    <li>https://statweb.stanford.edu/~jhf/ftp/trebst.pdf - In-depth theory of Gradient Boosting (Advanced)</li>\n",
    "</ul>\n",
    "\n",
    "<br /><br />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.datasets import load_boston\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the first part of this class we gonna use toy dataset that comes with Scikit-Learn library. The dataset is about predicting house prices in Boston using 13 continuous features: https://scikit-learn.org/stable/datasets/index.html\n",
    "\n",
    "These are the features of the data:\n",
    "\n",
    "CRIM - per capita crime rate by town\n",
    "\n",
    "ZN - proportion of residential land zoned for lots over 25,000 sq.ft.\n",
    "\n",
    "INDUS - proportion of non-retail business acres per town\n",
    "\n",
    "CHAS - Charles River dummy variable (= 1 if tract bounds river; 0 otherwise)\n",
    "\n",
    "NOX - nitric oxides concentration (parts per 10 million)\n",
    "\n",
    "RM - average number of rooms per dwelling\n",
    "\n",
    "AGE - proportion of owner-occupied units built prior to 1940\n",
    "\n",
    "DIS - weighted distances to five Boston employment centres\n",
    "\n",
    "RAD - index of accessibility to radial highways\n",
    "\n",
    "TAX - full-value property-tax rate per $10,000\n",
    "\n",
    "PTRATIO - pupil-teacher ratio by town\n",
    "\n",
    "B - 1000(Bk - 0.63)^2 where Bk is the proportion of blacks by town\n",
    "\n",
    "LSTAT - % lower status of the population\n",
    "\n",
    "TARGET (value we need to estimate) - Median value of owner-occupied homes in $1000â€™s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now load the data and do some EDA (Exploratory Data Analysis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading the data\n",
    "X, y = load_boston(return_X_y=True)\n",
    "\n",
    "# feature names\n",
    "cols = ['CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE', 'DIS', 'RAD', 'TAX', 'PTRATIO', 'B', 'LSTAT']\n",
    "\n",
    "# creating the dataframe for better visualizations and data processing\n",
    "df = pd.DataFrame(np.c_[X, y], columns=cols + ['TARGET']); df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's visualize distribution of each feature using boxplot\n",
    "\n",
    "We can see that feature scale highly differs, but, fortunately, tree-based models <b>doesn't require</b> feature re-scaling (subtract mean and divide by standard deviation or something similar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.plot(kind='boxb', rot=90, figsize=(15, 5))\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's see correlation (Pearson's correlation) matrix of features and target. \n",
    "We can observe that some of the features (e.g LSTAT, RM) are in <b>high correlation</b> with the target, that usually means that they are <b>good predictors</b> for estimating target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr = df.corr()\n",
    "sns.clustermap(corr, figsize=(15, 15), cmap='mako', method='centroid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# here we sort features in decreasing order of high correlation with target variable.\n",
    "\n",
    "np.abs(corr['TARGET']).sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets move on prediction.\n",
    "\n",
    "Let's imagine our goal is to optimize MSE (Mean Squred Error).\n",
    "\n",
    "In that case, trivial solution using constant predictor (always predicting same value irrespective of the input) would be the <b>sample mean</b> of targets in training dataset. One can proof mathematically, that for MSE the optimal constant predictor is indeed a mean. You can google it up :)\n",
    "\n",
    "Let's find out what the MSE is in this case (for simplicity we don't split the data and do everything on training set for now)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mse(x, y):\n",
    "    return np.mean((y - x) ** 2)\n",
    "\n",
    "constant_predictor = df['TARGET'].mean()\n",
    "\n",
    "mse(constant_predictor, df['TARGET'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see empirically that deviations from optimal constant predictor causes MSE to grow, i.e MSE is optimal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eps = 1\n",
    "\n",
    "mse(constant_predictor - eps, df['TARGET']), mse(constant_predictor + eps, df['TARGET'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, now let's try to do slightly better. \n",
    "\n",
    "Let's try to divide our training set in <b>two groups</b> and for each group, calculate <b>their own mean</b>. If we make this split in a smart way, intuitively the two means would be better than using just <b>single global mean</b> as we did in previous example.\n",
    "\n",
    "One way to do a split is to select on of the features of the data, let's say X, and some threshold, T, and make:<br /><br />\n",
    "group 1 - { all points having $X<T$ }<br />\n",
    "group 2 - {all points having $X>=T$}<br />\n",
    "\n",
    "Then we will have 2 constant predictors - one for each group - and, for every input we first gonna determine its group, and then use group's mean to estimate the target.\n",
    "\n",
    "This intuition is captured by Decision Tree with depth 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor, plot_tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining decision tree with depth 1.\n",
    "dt = DecisionTreeRegressor(max_depth=1)\n",
    "\n",
    "# fitting the decision tree on our data/\n",
    "model = dt.fit(X, y)\n",
    "\n",
    "# visualizing the tree\n",
    "plot_tree(model, feature_names=cols, proportion=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we see, decision tree chose a feature \"RM\" and a threshold T=6.941 to do the split. You can also see the proportion of samples in two groups and value of the mean used for prediction.\n",
    "\n",
    "Let's verify this picture using python by our own:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "left = df[df['RM'] < 6.941]['TARGET']\n",
    "right = df[df['RM'] > 6.941]['TARGET']\n",
    "\n",
    "left.mean(), right.mean(), mse(left.mean(), left), mse(right.mean(), right)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Intuitively, we can go on with splitting and apply same splitting logic recursively on groups (leaf nodes)...\n",
    "\n",
    "For example, the tree with depth=2 can be seen below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = DecisionTreeRegressor(max_depth=2)\n",
    "model = dt.fit(X, y)\n",
    "plot_tree(model, feature_names=cols, proportion=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>General notes on Decision Trees for Regression</b>\n",
    "\n",
    "<ul>\n",
    "    <li>Generally, there is no need to use binary trees (i.e we can have three groups instead of two) and no need to select single feature for splitting. However, the approach we used is most widely used and popular.</li>\n",
    "    <li>In practice, trees experience <b>low bias and very high variance</b> - i.e they tend to overfit. One of the ways to control tree complexity is to limit it's depth or to set the restriction on the minimal number of datapoints required for node to be split further. Usually, ensembling approaches (follow up in this class) are used to lower the variance.</li>\n",
    "    <li>For <b>nominal categorical variables</b> (i.e ones that can't be ordered) splitting is done by partitioning the set of values for the variable. For example, if we have X={A, B, C} than we can have {A} {B, C} as a split.</li>\n",
    "</ul>\n",
    "\n",
    "Refer here, for more notes: https://scikit-learn.org/stable/modules/tree.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To be continued..."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {
    "height": "164px",
    "width": "514px"
   },
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
