{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Underfitting, Overfitting and Regularization in ML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intro"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<br /><br />\n",
    "\n",
    "### Motivation\n",
    "\n",
    "There are quite a few Machine Learning algorithms out there and we need the one that best suits the problem we are trying to solve. \n",
    "\n",
    "The most important criteria one should consider while selecting a model is its <b>generalization ability</b> or, more formally, an <b>Expected Test Error</b>. Model generalization ability means how well does the model perform on <b>unseen data</b> (i.e outside the training set) when faced real world.\n",
    "\n",
    "Imagine our task is to say whether or not there is a dog depicted on the picture. Also, let's assume our train dataset cotains limited number of dog breeds. Example of a model with good generalization ability would be a one that performs accurately even on the pictures with dog breeds that never occured in train set.\n",
    "\n",
    "Throughout the class, we will go through techniques for estimating and improving model generalizability. Theory will be backed up by examples in Python and Scikit-Learn."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "### Learning Objectives\n",
    "\n",
    "At the end of this class, listeners will be able to:\n",
    "\n",
    "<ul>\n",
    "    <li>Understand concepts related to model generalization such as overfitting, underfitting and bias-variance trade-off.</li>\n",
    "    <li>Techniques to improve generalization by regularization.</li>\n",
    "    <li>Apply regularization to linear and logistic regression.</li>\n",
    "    <li>Use Scikit-Learn library for simple tasks.</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Reading Material\n",
    "\n",
    "https://web.stanford.edu/~hastie/Papers/ESLII.pdf The Elements of Statistical Learning, section 7.<br />\n",
    "https://en.wikipedia.org/wiki/Bias%E2%80%93variance_tradeoff<br />\n",
    "https://scikit-learn.org/stable/modules/linear_model.html#lasso\n",
    "<br /><br />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Underfitting and Overfitting\n",
    "\n",
    "Below is an intuitive example-by-example explanation of the two essential concepts in Machine Learning: <b>Underfitting</b> and <b>Overfitting</b>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "In below examples, we are going to use toy linear regression problem. \n",
    "\n",
    "We are going to predict a <b>land price</b> (for simplicity, suppose its quadratic, i.e X by X square meters) given the <b>length of its side</b> (in meters).\n",
    "\n",
    "For simplicity, we are going to use synthetic (not real) data. We assume that true dependence between the length of land's side and its price is given by parabola: $f(x)=a*x^2+b*x+c, x>0$ where $x$ is length and $f(x)$ is price.\n",
    "\n",
    "We also assume that our observations are <b>noisy</b> and we simulate this noise by adding centered Gaussian to our parabola.\n",
    "\n",
    "Let's generate some data for illustrations..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A, B, C = 3.7, 0.1, 2.1 # True parabola parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "def generate_training_set(a, b, c, size: int, eps: float=2):\n",
    "    \n",
    "    # define quadratic function of form f(x)=A*X^2+B*X+C\n",
    "    f = lambda x: a*x**2 + b*x + c\n",
    "    \n",
    "    # define X axis\n",
    "    x = np.linspace(0.1, 10, num=size)\n",
    "    \n",
    "    # define Y axis\n",
    "    y = [f(i) for i in x]\n",
    "    \n",
    "    # generate random Gaussian noise\n",
    "    e = np.random.normal(0, eps, size=(size))\n",
    "    \n",
    "    return np.array(x), np.array(y), e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# plot the dataset (each run will plot different dataset from the same distribution/data generating process)\n",
    "\n",
    "X, Y, e = generate_training_set(A, B, C, size=500, eps=20)\n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.scatter(X, Y + e, s=2)\n",
    "plt.plot(X, Y, 'g')\n",
    "plt.xlabel(\"Length of land's side\")\n",
    "plt.ylabel(\"Land's price\")\n",
    "plt.title('Data (Green line is true data, dots are noisy observations)')\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's fit the above data using just $x$ (Length of land's side) as a single feature:\n",
    "\n",
    "Our model will be $\\hat{f}(x)=w_0 + w_1 * x$<br /><br />\n",
    "Or, in vector notation: $\\hat{f}(x)=dot([w_0, w_1],[1, x])$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining linear regression model (fit_intercept=False means disabling bias term)\n",
    "\n",
    "model = LinearRegression(fit_intercept=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, Y_true, e = generate_training_set(A, B, C, size=20, eps=20)\n",
    "\n",
    "Y = Y_true + e # creating noisy observations by adding noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding dummy column for bias\n",
    "\n",
    "X1 = np.c_[np.ones(X.shape[0]), X]\n",
    "X1[0:5], Y[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X1, Y) # fitting the model using OLS\n",
    "model.coef_ # displaying learned weights: w_0 and w_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(X1) # generating predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting learned model \n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.scatter(X, Y, s=30)\n",
    "plt.plot(X, Y_true, 'g')\n",
    "plt.plot(X, y_pred, 'r')\n",
    "plt.title('Dots - Observations; Green Line - True data; Red Line - Our Estimate')\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obviously, previous model is too simple and doesn't have enough \"power\" to capture quadratic dependence. To mitigate this, we introduce additional features to help model. \n",
    "\n",
    "We gonna use <b>polynomial features</b> up to 10-th degree and our model will be:<br />\n",
    "\n",
    "$\\hat{f}(x)=w_0 + w_1 * x + w_2 * x^2 + w_3 * x^3 + w_4 * x^4 + w_5 * x^5 + w_6 * x^6 + w_7 * x^7 + w_8 * x^8 + w_9 * x^9 + w_10 * x^{10}$\n",
    "\n",
    "<b>Important Note: </b>Despite the fact that we add polynomial features, model stays linear because linearity is relative to weights and not features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to use Scikit-Learn's helper class for generating new features: \n",
    "\n",
    "`from sklearn.preprocessing import PolynomialFeatures`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's first define this helper class\n",
    "\n",
    "poly = PolynomialFeatures(degree=10, include_bias=False)\n",
    "\n",
    "# let's create features...\n",
    "\n",
    "X2 = poly.fit_transform(X.reshape(-1, 1))\n",
    "\n",
    "X2[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LinearRegression(fit_intercept=False) \n",
    "\n",
    "model.fit(X2, Y) # fitting again with new feature set\n",
    "model.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(X2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 10))\n",
    "plt.scatter(X, Y, s=30)\n",
    "plt.plot(X, Y_true, 'g')\n",
    "plt.plot(X, y_pred, 'r')\n",
    "plt.title('Dots - Observations; Green Line - True data; Red Line - Our Estimate')\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Some Observations...</b>\n",
    "    \n",
    "<ul>\n",
    "    <li>Very <b>simple</b>  models have poor performance due to lack of expressive power to learn data distribution.</li>\n",
    "    <li>Very <b>complex</b> models have poor performance due to excessive expressive power leading to fitting <b>noise</b> instead of the real data.</li>\n",
    "</ul>\n",
    "    \n",
    "The first one is called <b>Underfitting</b> and the second one - <b>Overfitting</b>\n",
    "\n",
    "In above example, first simple model <b>underfitted</b> our data, whereas, the second complex one - <b>overfitted</b>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bias-Variance Trade-Off\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will continue with our toy example...\n",
    "\n",
    "When talking about model generalization ability, one of the important questions to ask is the following:\n",
    "\n",
    "When training our model on <b>different training sets</b> (drawn from the <b>same</b> underlying data distribution, parabola in our case), what is our model's <b>expected prediction</b> for arbitrary fixed data point <b>$X=x_0$</b> (also drawn from underlying data distribution)?\n",
    "\n",
    "Arguably, same model trained on different datasets will produce different predictions for same fixed $x_0$. The question is, how can we <b>quantify</b> the <b>amount of fluctuations</b> of prediction around it's true value on different datasets?\n",
    "\n",
    "Below is the illustrative example of these statements..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_and_predict(x, use_first_model=True):\n",
    "    \"\"\"This function generates training data from our initial toy distribution, trains a model (either first or \n",
    "    second one from above section) and generates a prediction for given single data point X.    \n",
    "    \n",
    "    Returns true value and model's estimate\n",
    "    \"\"\"\n",
    "\n",
    "    X, Y_true, e = generate_training_set(A, B, C, size=20, eps=20)\n",
    "\n",
    "    Y = Y_true + e # creating noisy observations by adding Gaussian noise (this is our train data)\n",
    "    \n",
    "    poly = PolynomialFeatures(degree=(1 if use_first_model else 10), include_bias=True)\n",
    "    \n",
    "    # augmenting training data with polynomial features\n",
    "    X1 = poly.fit_transform(X.reshape(-1, 1))\n",
    "\n",
    "    model = LinearRegression(fit_intercept=False) \n",
    "\n",
    "    model.fit(X1, Y) # fitting the model using OLS\n",
    "    \n",
    "    return A*x**2 + B*x + C, model.predict(poly.fit_transform([[x]]))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# here we train model on different datasets and calculate expected prediction and it's fluctuation (variance).\n",
    "\n",
    "N = 1000 # we train model on N different datasets in total.\n",
    "\n",
    "x0 = 5 # this is our fixed data point on which we will measure prediction quality and fluctuations.\n",
    "\n",
    "use_first_model=True # boolean flag wheather to use first (simple) model or second (simple) model.\n",
    "\n",
    "y_preds = []\n",
    "    \n",
    "for _ in range(1000):\n",
    "        \n",
    "    y_true, y_pred = fit_and_predict(x0, use_first_model=True)\n",
    "        \n",
    "    y_preds.append(y_pred)\n",
    "\n",
    "print('True Value: ', y_true)\n",
    "print('Average Estimate: ', np.mean(y_pred))\n",
    "print('Bias: ', y_true - np.mean(y_pred))\n",
    "print('Variance: ', np.mean((y_preds - np.mean(y_pred))**2))\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize=(15, 5))\n",
    "sns.distplot(y_preds, ax=ax[0])\n",
    "ax[1].scatter([0]*len(y_preds), y_preds, s=0.1)\n",
    "ax[1].scatter([0], [y_true], s=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# here we train model on different datasets and calculate expected prediction and it's fluctuation (variance).\n",
    "\n",
    "N = 1000 # we train model on N different datasets in total.\n",
    "\n",
    "x0 = 5 # this is our fixed data point on which we will measure prediction quality and fluctuations.\n",
    "\n",
    "use_first_model=True # boolean flag wheather to use first (simple) model or second (simple) model.\n",
    "\n",
    "y_preds = []\n",
    "    \n",
    "for _ in range(1000):\n",
    "        \n",
    "    y_true, y_pred = fit_and_predict(x0, use_first_model=False)\n",
    "        \n",
    "    y_preds.append(y_pred)\n",
    "\n",
    "print('True Value: ', y_true)\n",
    "print('Average Estimate: ', np.mean(y_pred))\n",
    "print('Bias: ', y_true - np.mean(y_pred))\n",
    "print('Variance: ', np.mean((y_preds - np.mean(y_pred))**2))\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize=(15, 5))\n",
    "sns.distplot(y_preds, ax=ax[0])\n",
    "ax[1].scatter([0]*len(y_preds), y_preds, s=0.1)\n",
    "ax[1].scatter([0], [y_true], s=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we saw from the examples, first (simple) model's prediction on given $x_0$ systematically deviated from it's true value, whereas, complex model's prediction was centered around it's true value. We call this phenomenon model's <b/>bias</b> - measure of deviation from true value. \n",
    "\n",
    "Formally, $Bias(\\hat{f}(x_0))=f(x_0)-\\mathbb{E}[\\hat{f}(x_0)]\\$, where expectation is over different training sets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also can observe, that first (simple) model's predictions where <b>less fluctuating</b> over different training sets as opposed to second (complex) model where fluctuations were higher.\n",
    "\n",
    "We measure a fluctuations using <b>variance</b>: $Var(\\hat{f}(x_0))=\\mathbb{E}[(\\hat{f}(x_0)-\\mathbb{E}[\\hat{f}(x)])^2]$\n",
    "\n",
    "<br />\n",
    "<br />\n",
    "\n",
    "<b>Some More Observations...</b>\n",
    "    \n",
    "<ul>\n",
    "    <li><b>Overfitting</b> (case of complex model above) is usually associated with <b>small bias</b> and <b>high variance</b></li>\n",
    "    <li><b>Underfitting</b> (case of simple model above) is usually associated with <b>high bias</b> and <b>small variance</b></li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<big><b>Important Fact:</b></big>\n",
    "\n",
    "Usually in Machine Learning (except sometimes in Deep Learning) there is a trade-off between model's Bias and Variance. \n",
    "\n",
    "As we gradually grow model's complexity, we gradually reduce bias and increase variance. \n",
    "\n",
    "The goal is to find a sweet spot where both bias and variance are acceptable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<big><b>Theorem (Bias-Variance Decomposition)</b></big>\n",
    "\n",
    "Whichever model $\\hat{f}$ we select, we can decompose its expected error (quadratic risk / mean squared error in practice) on an arbitrary data point $x_0$ as follows:\n",
    "\n",
    "$\\mathbb{E}\\Big[\\big(f(x_0) - \\hat{f}(x_0)\\big)^2\\Big]\n",
    "= \\Big(\\operatorname{Bias}\\big[\\hat{f}(x_0)\\big] \\Big) ^2 + \\operatorname{Var}\\big[\\hat{f}(x_0)\\big] + \\sigma^2$\n",
    "\n",
    "<br />\n",
    "\n",
    "where $\\sigma^2$ is an <b>irreducible error</b> which we can't get rid off. This error is usually induced by error in observations in our data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Cross-Validation</b> is usually a good method for estimating expected error of a model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Estimating expected error is, in some sense, a good way to estimate model's generalization ability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From previous sections we saw that we need to avoid both underfitting and overfitting. \n",
    "\n",
    "Underfitting can be avoiding simply by increasing model compexity, but then it can overfit.\n",
    "\n",
    "<b>Regularization</b> in Machine Learning is a set of techniques usually involving imposing certain constraints on model's structure to prevent fitting noisy points in data.\n",
    "\n",
    "This section will cover so called <b>L1 and L2 regularization techniques</b>, which basically involves constraining model weights to grow large. Particularly, we add extra component to loss function, that is, an L1 or L2 norm of the weights.\n",
    "\n",
    "$NewLoss(\\hat{y}, y)=Loss(\\hat{y}, y) + \\alpha*\\| x\\|_2^2$ in case of L2 Regularization<br /><br />\n",
    "$NewLoss(\\hat{y}, y)=Loss(\\hat{y}, y) + \\alpha*| x|$ in case of L1 Regularization\n",
    "\n",
    "$\\alpha$ is <b>hyperparameter</b> controlling the strength of regularization effect.\n",
    "\n",
    "Linear Regressions with L2 and L1 regularization are called <b>Ridge</b> and <b>Lasso</b> Regressions, respectively.\n",
    "\n",
    "Loss functions of Ridge and Lasso regressions are still <b>Convex</b> functions and have global optima (proof is beyond this class). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing Ridge and Lasso Regressions from Scikit-Learn\n",
    "\n",
    "from sklearn.linear_model import Ridge, Lasso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_and_predict(alpha, ridge=True):\n",
    "    \"\"\"Same function as used above, just with small modifications.\n",
    "    \n",
    "    Here we use polynomial features up to 6-th degree and test Ridge regression with different hyperparameters\n",
    "    alpha.\n",
    "    \n",
    "    We observe how the L2 norm of weights changes with different settings for alpha.\n",
    "    \"\"\"\n",
    "\n",
    "    X, Y_true, e = generate_training_set(A, B, C, size=20, eps=20)\n",
    "\n",
    "    Y = Y_true + e # creating noisy observations by adding Gaussian noise (this is our train data)\n",
    "    \n",
    "    poly = PolynomialFeatures(degree=6, include_bias=True)\n",
    "    \n",
    "    # augmenting training data with polynomial features\n",
    "    X1 = poly.fit_transform(X.reshape(-1, 1))\n",
    "\n",
    "    if ridge:\n",
    "        model = Ridge(alpha=alpha, fit_intercept=False) \n",
    "    else:\n",
    "        model = Lasso(alpha=alpha, fit_intercept=False)\n",
    "\n",
    "    model.fit(X1, Y) # fitting the model using OLS\n",
    "    print(\"L2 Norm of model weights:\", np.sum(model.coef_**2))\n",
    "    print('\\nweights:')\n",
    "    print('\\n'.join('w%d=%.3f' % (i, w) for i, w in enumerate(model.coef_)))\n",
    "    \n",
    "    plt.figure(figsize=(10, 10))\n",
    "    plt.scatter(X, Y, s=30)\n",
    "    plt.plot(X, Y_true, 'g')\n",
    "    plt.plot(X, model.predict(X1), 'r')\n",
    "    plt.title('Dots - Observations; Green Line - True data; Red Line - Our Estimate')\n",
    "    plt.grid()    \n",
    "\n",
    "fit_and_predict(100, ridge=True)\n",
    "#fit_and_predict(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Some Observations...</b>\n",
    "\n",
    "<ul>\n",
    "<li>Both types of regularization make weights smaller in magnitude depending on the $\\alpha$ hyperparameter.</li>\n",
    "<li>Lasso Regression introduces <b>sparsity</b> in model weights, which might be useful sometimes.</li>\n",
    "    <li>Lasso objective is <b>non-differentiable</b> (due to abs() function) and can't be solved in a same way as OLS solution. Scikit-Learn uses iterative coordinate ascent algorithm (details are beyond this class).</li>\n",
    "</ul>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {
    "height": "164px",
    "width": "514px"
   },
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
