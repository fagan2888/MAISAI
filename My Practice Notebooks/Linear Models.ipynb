{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In This notebook, I will test linear models such:\n",
    "\n",
    "    * Simple Linear Regression\n",
    "    * Multiple Linear Regression\n",
    "    * Ridge\n",
    "    * Lasso\n",
    "    * Elatic Net\n",
    "\n",
    "[Sklearn Documentation](https://scikit-learn.org/stable/modules/linear_model.html#)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In regression the target value is expected to be a linear combination of the features. In mathematical notation, if $\\hat{y}$ is the predicted value.\n",
    "\n",
    "$$\n",
    "\\hat{y}(w, x) = w_0 + w_1 x_1 + ... + w_p x_p\n",
    "$$\n",
    "\n",
    "Where, $w = (w_1,..., w_p)$ is the vecotr of coefficients and $w_0$ is the intercept\n",
    "\n",
    "Linear Regression fits a linear model with coefficients $w = (w_1, ..., w_p)$ to minimize the residual sum of squares between the observed targets in the dataset, and the targets predicted by the linear approximation. Mathematically it solves a problem of the form:\n",
    "\n",
    "$$\n",
    "\\min_{w} || X w - y||_2^2\n",
    "$$\n",
    "\n",
    "\n",
    "**In case of Simple Linear Regression we have one independent variable or feature**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This example uses the only the first feature of the diabetes dataset, in order to illustrate a two-dimensional plot of this regression technique. The straight line can be seen in the plot, showing how linear regression attempts to draw a straight line that will best minimize the residual sum of squares between the observed responses in the dataset, and the responses predicted by the linear approximation.\n",
    "\n",
    "The coefficients, the residual sum of squares and the coefficient of determination are also calculated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Import linear model and dataset\n",
    "from sklearn import datasets, linear_model\n",
    "\n",
    "# Import performance metrics\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# Import test_train_split\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "\n",
    "X, y = datasets.load_diabetes(return_X_y=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# As this is a Simple Linear Regression we only need one feature or independent variable\n",
    "\n",
    "X = X[:, np.newaxis, 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split X and y into train and test, with test size 20%\n",
    "\n",
    "train_x, test_x, train_y, test_y = train_test_split(X, y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Simple Linear Regression Model\n",
    "\n",
    "reg = linear_model.LinearRegression(fit_intercept=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "\n",
    "result = reg.fit(train_x, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make prediction on the test set\n",
    "\n",
    "y_pred = result.predict(test_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the coefficients\n",
    "\n",
    "print(\"Intercept is: {}, and coeffient estimate is {}\".format(np.round(result.intercept_, 3), np.round(result.coef_,3)))\n",
    "print()\n",
    "print(\"Mean Squared Error is {}\".format(np.round(mean_squared_error(test_y, y_pred),3)))\n",
    "print()\n",
    "print(\"R_Squared is {}\".format(np.round(r2_score(test_y, y_pred),3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let make some plots\n",
    "\n",
    "\n",
    "plt.scatter(test_x, test_y,  color='black')\n",
    "plt.plot(test_x, y_pred, color='blue', linewidth=3)\n",
    "\n",
    "plt.xticks(())\n",
    "plt.yticks(())\n",
    "\n",
    "\n",
    "plt.axhline(y=0, color='k')\n",
    "plt.axvline(x=0, color='k')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multiple Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will use the same dataset with full set of features as this is the example of multiple linear regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "\n",
    "X, y = datasets.load_diabetes(return_X_y=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split X and y into train and test, with test size 20%\n",
    "\n",
    "train_x, test_x, train_y, test_y = train_test_split(X, y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Multiple Linear Regression Model\n",
    "\n",
    "reg = linear_model.LinearRegression(fit_intercept=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "\n",
    "result = reg.fit(train_x, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make prediction on the test set\n",
    "\n",
    "y_pred = result.predict(test_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the coefficients\n",
    "\n",
    "print(\"Intercept is: {}\".format(np.round(result.intercept_, 3)))\n",
    "print()\n",
    "print(\"Coeficient estimates are {}\".format(np.round(result.coef_, 3)))\n",
    "print()\n",
    "print(\"Mean Squared Error is {}\".format(np.round(mean_squared_error(test_y, y_pred),3)))\n",
    "print()\n",
    "print(\"R_Squared is {}\".format(np.round(r2_score(test_y, y_pred),3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ridge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ridge regression addresses some of the problems of Ordinary Least Squares by imposing a penalty on the size of the coefficients. The ridge coefficients minimize a penalized residual sum of squares:\n",
    "\n",
    "$$\n",
    "\\min_{w} || X w - y||_2^2 + \\alpha ||w||_2^2\n",
    "$$\n",
    "\n",
    "The complexity parameter $\\alpha \\geq 0$ controls the amount of shrinkage: the larger the value of $\\alpha$,  the greater the amount of shrinkage and thus the coefficients become more robust to collinearity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**I will use the same dataset, with full feature set**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "\n",
    "X, y = datasets.load_diabetes(return_X_y=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split X and y into train and test, with test size 20%\n",
    "\n",
    "train_x, test_x, train_y, test_y = train_test_split(X, y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Ridge Regression Model. This is a Ridge regression with built-in cross-validation.\n",
    "# I indicated 10-fold cross-validation\n",
    "\n",
    "reg = linear_model.RidgeCV(alphas=[1e-3, 1e-2, 1e-1, 1, 10], cv=10, fit_intercept=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "\n",
    "result = reg.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print model output\n",
    "\n",
    "print(\"Model intercept is {}\".format(np.round(result.intercept_, 3)))\n",
    "print()\n",
    "print(\"Coefienct estimates are {}\".format(np.round(result.coef_, 3)))\n",
    "print()\n",
    "print(\"Best alpha is {}\".format(result.alpha_))\n",
    "print()\n",
    "print(\"R_sqaured is {}\".format(np.round(result.score(test_x, test_y), 3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lasso"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Lasso is a linear model that estimates sparse coefficients. It is useful in some contexts due to its tendency to prefer solutions with fewer non-zero coefficients, effectively reducing the number of features upon which the given solution is dependent. For this reason Lasso and its variants are fundamental to the field of compressed sensing. Under certain conditions, it can recover the exact set of non-zero coefficients.\n",
    "\n",
    "Mathematically, it consists of a linear model with an added regularization term. The objective function to minimize is:\n",
    "\n",
    "$$\n",
    "\\min_{w} { \\frac{1}{2n_{\\text{samples}}} ||X w - y||_2 ^ 2 + \\alpha ||w||_1}\n",
    "$$\n",
    "\n",
    "\n",
    "The lasso estimate thus solves the minimization of the least-squares penalty with $\\alpha ||w||_1$ added, where $\\alpha$ is a constant and $||w||_1$ is the $\\ell_1$-norm of the coefficient vector. The alpha parameter controls the degree of sparsity of the estimated coefficients."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**I will use the same dataset, with full feature set**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "\n",
    "X, y = datasets.load_diabetes(return_X_y=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split X and y into train and test, with test size 20%\n",
    "\n",
    "train_x, test_x, train_y, test_y = train_test_split(X, y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Lasso Regression Model. This is a Lasso regression with built-in cross-validation.\n",
    "# I indicated 10-fold cross-validation\n",
    "\n",
    "reg = linear_model.LassoCV(alphas=[1e-3, 1e-2, 1e-1, 1, 10], cv=10, fit_intercept=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "\n",
    "result = reg.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print model output\n",
    "\n",
    "print(\"Model intercept is {}\".format(np.round(result.intercept_, 3)))\n",
    "print()\n",
    "print(\"Coefienct estimates are {}\".format(np.round(result.coef_, 3)))\n",
    "print()\n",
    "print(\"Best alpha is {}\".format(result.alpha_))\n",
    "print()\n",
    "print(\"R_sqaured is {}\".format(np.round(result.score(test_x, test_y), 3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Elatic-Net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ElasticNet is a linear regression model trained with both $\\ell_1$ and $\\ell_2$-norm regularization of the coefficients. This combination allows for learning a sparse model where few of the weights are non-zero like Lasso, while still maintaining the regularization properties of Ridge.We control the convex combination of $\\ell_1$ and $\\ell_2$ using the l1_ratio parameter.\n",
    "\n",
    "\n",
    "Elastic-net is useful when there are multiple features which are correlated with one another. Lasso is likely to pick one of these at random, while elastic-net is likely to pick both.\n",
    "\n",
    "A practical advantage of trading-off between Lasso and Ridge is that it allows Elastic-Net to inherit some of Ridgeâ€™s stability under rotation.\n",
    "\n",
    "\n",
    "The objective function to minimize is in this case:\n",
    "\n",
    "$$\n",
    "\\min_{w} { \\frac{1}{2n_{\\text{samples}}} ||X w - y||_2 ^ 2 + \\alpha \\rho ||w||_1 +\n",
    "\\frac{\\alpha(1-\\rho)}{2} ||w||_2 ^ 2}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**I will use the same dataset, with full feature set**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "\n",
    "X, y = datasets.load_diabetes(return_X_y=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split X and y into train and test, with test size 20%\n",
    "\n",
    "train_x, test_x, train_y, test_y = train_test_split(X, y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Elatic-Net Regression Model. This is a Elatic-Net regression with built-in cross-validation.\n",
    "# I indicated 10-fold cross-validation.\n",
    "# The class ElasticNetCV can be used to set the parameters alpha and l1_ratio by cross-validation.\n",
    "\n",
    "reg = linear_model.ElasticNetCV(l1_ratio=[1e-4,1e-3,1e-2,.1, .5, .7, .9, .95, .99, 1],\n",
    "                                alphas=[1e-3, 1e-2, 1e-1, 1, 10], \n",
    "                                cv=10, fit_intercept=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "\n",
    "result = reg.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print model output\n",
    "\n",
    "print(\"Model intercept is {}\".format(np.round(result.intercept_, 3)))\n",
    "print()\n",
    "print(\"Coefienct estimates are {}\".format(np.round(result.coef_, 3)))\n",
    "print()\n",
    "print(\"Best alpha is {}\".format(result.alpha_))\n",
    "print()\n",
    "print(\"Best l1_ration is {}\".format(result.l1_ratio_))\n",
    "print()\n",
    "print(\"R_sqaured is {}\".format(np.round(result.score(test_x, test_y), 3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
