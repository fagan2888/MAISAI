{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create anaconda environment\n",
    "<br>\n",
    "```bash\n",
    "conda create -n ml python=3.7.5 jupyter\n",
    "```\n",
    "Install fastai library\n",
    "<br>\n",
    "```bash\n",
    "conda install -c pytorch -c fastai fastai\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neural networks can be considered as cascade or pipeline of linear classifiers or regressors. For instance:\n",
    "Let $X \\in \\mathbb{R}^m$ be an our data and $Y \\in \\mathbb{R}^n$ be a classes. Define $H \\in \\mathbb{R}^K$ and $f_{i}:X \\to H_i$ is a linear function:\n",
    "<br>\n",
    "$$f_{i} = \\sum_{j=1}^kW_{i,j}x_j + b_i$$ or \n",
    "<br>\n",
    "$$f_i = W_ix + b$$\n",
    "<br>\n",
    "where \n",
    "$\\begin{align}\n",
    "    x &= \\begin{pmatrix}\n",
    "           x_{1} \\\\\n",
    "           x_{2} \\\\\n",
    "           \\vdots \\\\\n",
    "           x_{m}\n",
    "         \\end{pmatrix} \\in \\mathbb{R}^{1 \\times m}\n",
    "  \\end{align}$, \n",
    "$\\begin{align}\n",
    "    b &= \\begin{pmatrix}\n",
    "           b_{1} \\\\\n",
    "           b_{2} \\\\\n",
    "           \\vdots \\\\\n",
    "           b_{n}\n",
    "         \\end{pmatrix} \\in \\mathbb{R}^{1 \\times n}\n",
    "  \\end{align}$ \n",
    "and $W_i = (W_{i,1}, W_{i,2}, \\dots, W_{i,m}) \\in \\mathbb{R}^{m \\times 1}$\n",
    "<br>\n",
    "$$\n",
    "f(x) = Wx + b\n",
    "$$\n",
    "<br>\n",
    "here \n",
    "$\\begin{align}\n",
    "    x &= \\begin{pmatrix}\n",
    "           x_{1} \\\\\n",
    "           x_{2} \\\\\n",
    "           \\vdots \\\\\n",
    "           x_{m}\n",
    "         \\end{pmatrix} \\in \\mathbb{R}^{1 \\times m}\n",
    "  \\end{align}$, \n",
    " $\\begin{align}\n",
    "    b &= \\begin{pmatrix}\n",
    "           b_{1} \\\\\n",
    "           b_{2} \\\\\n",
    "           \\vdots \\\\\n",
    "           b_{n}\n",
    "         \\end{pmatrix} \\in \\mathbb{R}^{1 \\times n}\n",
    "  \\end{align}$\n",
    " and\n",
    "$\\begin{align}\n",
    "    W &= \\begin{pmatrix}\n",
    "           W_{1, 1}, W_{1, 2} \\dots W_{1, m} \\\\\n",
    "           W_{2, 1}, W_{2, 2} \\dots W_{2, m} \\\\\n",
    "           \\vdots \\\\\n",
    "           W_{n,1}, W_{n, 2} \\dots W_{n, m}\n",
    "         \\end{pmatrix} \\in \\mathbb{R}^{n \\times m}\n",
    " \\end{align}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now consider other mapping $\\sigma:H \\to A$ where $A \\in \\mathbb{R}^n$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\sigma(x) = \\frac{1}{1+e^{-x}}$$\n",
    "Sigmoid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\sigma(x)=\\frac{1-e^{-x}}{1+e^{-x}}$$\n",
    "Tahn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\sigma(x) = max(0, x)$$\n",
    "ReLu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why DNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why deep neural networks?\n",
    "- Dimesionality\n",
    "- Multi model (enssembple)\n",
    "- Features extractor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Still why should they work?\n",
    "- Needs more data\n",
    "- Computationaly expensive training and inference\n",
    "- Black box"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In case of kernel methods, linear regression, random forest or gradient boosting, exists analysis why model should work. But for DNN we don't have such a vivid imagination."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Universal Approximation Theorems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Theorem (The Universal Approximation Theorem):\n",
    "<br>\n",
    "For every $\\sigma:\\mathbb{R}\\to\\mathbb{R}$ bounded, and continuous function (called the activation function). Let $I_m$ denote the m-dimensional unit hypercube $[0,1]^m$ The space of real-valued continuous functions on \n",
    "$I_{m}$ is denoted by \n",
    "$C(I_{m})$. Then, given any $\\varepsilon >0$ and any function $f\\in C(I_{m})$, there exist an integer $N$, real constants $v_{i},b_{i}\\in \\mathbb {R}$ and real vectors $w_{i}\\in \\mathbb {R} ^{m}$ for $i=1,\\ldots ,N$, such that we may define:\n",
    "<br>\n",
    "$$\n",
    "F( x ) = \\sum_{i=1}^{N} v_i \\sigma \\left( w_i^T x + b_i\\right)\n",
    "$$\n",
    "<br>\n",
    "as an approximate realization of the function $f$; that is,\n",
    "<br>\n",
    "$$\n",
    "|F(x)-f(x)|<\\varepsilon\n",
    "$$\n",
    "<br>\n",
    "for all $x\\in I_{m}$. In other words, functions of the form $F(x)$ are dense in $\\displaystyle C(I_{m})$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Theorem (The Universal Approximation Theorem for any Compact)\n",
    "<br>\n",
    "For every $\\sigma:\\mathbb{R}\\to\\mathbb{R}$ bounded, and continuous function (called the activation function). Let $K \\in \\mathbb{R}^m$ denote the any compact in $\\mathbb{R}^m$ The space of real-valued continuous functions on \n",
    "$K$ is denoted by \n",
    "$C(K)$. Then, given any $\\varepsilon >0$ and any function $f\\in C(K)$, there exist an integer $N$, real constants $v_{i},b_{i}\\in \\mathbb {R}$ and real vectors $w_{i}\\in \\mathbb {R} ^{m}$ for $i=1,\\ldots ,N$, such that we may define:\n",
    "<br>\n",
    "$$\n",
    "F( x ) = \\sum_{i=1}^{N} v_i \\sigma \\left( w_i^T x + b_i\\right)\n",
    "$$\n",
    "<br>\n",
    "as an approximate realization of the function $f$; that is,\n",
    "<br>\n",
    "$$\n",
    "|F(x)-f(x)|<\\varepsilon\n",
    "$$\n",
    "<br>\n",
    "for all $x\\in I_{m}$. In other words, functions of the form $F(x)$ are dense in $\\displaystyle C(K)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Theorem (Bounded case)\n",
    "<br>\n",
    "The universal approximation theorem for width-bounded networks can be expressed mathematically as follows:\n",
    "\n",
    "For any Lebesgue-integrable function \n",
    "$f:\\mathbb {R} ^{n}\\rightarrow \\mathbb {R}$ and any $\\epsilon >0$, there exists a fully-connected ReLU network \n",
    "$\\mathcal {A}$ with width $d_{m}\\leq {n+4}$, such that the function \n",
    "$F_{\\mathcal {A}}$ represented by this network satisfies\n",
    "<br>\n",
    "$$ \n",
    "\\int _{\\mathbb {R} ^{n}}\\left|f(x)-F_{\\mathcal {A}}(x)\\right|\\mathrm {d} x<\\epsilon\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definitions and Notions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets define weights per layer $l$ as $W^l$:\n",
    "<br>\n",
    "$$\n",
    "\\begin{align}\n",
    "    W^l &= \\begin{pmatrix}\n",
    "           W_{1, 1}^l, W_{1, 2}^l \\dots W_{1, m^l}^l \\\\\n",
    "           W_{2, 1}^l, W_{2, 2}^l \\dots W_{2, m^l}^l \\\\\n",
    "           \\vdots \\\\\n",
    "           W_{n^l,1}^l, W_{n^l, 2}^l \\dots W_{n^l, m^l}^l\n",
    "         \\end{pmatrix} \\in \\mathbb{R}^{n^l \\times m^l}\n",
    " \\end{align}\n",
    "$$\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "F(x) = \\sigma(W^{L-1}(\\dots \\sigma(W^2\\sigma(W_i^1x + b^1) + b^2)\\dots)) + b^{L-1}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We denote \n",
    "$$a^l = \\sigma(W^la^{l-1} + b^l)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we have a $n^{L-1}$ (hyperparameter alarm) dimensional vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Different Architectures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Weights sharing:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Residual connections:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recurrent neural networks (RNN):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LSTM, GRU Gates:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Deep Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Problem with $\\frac{\\partial L}{\\partial W_{i, j}^{l}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
